# -*- coding: utf-8 -*-
"""data 4381

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zDPgw5ggCRghtYwQKKBAkmnfQ7RIS8kN

#package imports
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import folium
import pandas as pd
from tabulate import tabulate

"""#data set loading, data understanding"""

df = pd.read_csv('restaurant.csv')

null_percentage = df.isnull().mean() * 100
print(null_percentage)

print(df.columns)

print(df.head)

df.info()

id_count = df['business_id'].nunique()
print(f'There are {id_count} unique IDs')

#this ensures that all values are string values
df['business_name'] = df['business_name'].astype(str)

#this creates the conditions to add a new category column based on business type
conditions = [
    df['business_name'].str.contains('grocery|market|stand|deli', case=False, na=False),
    df['business_name'].str.contains('school|university|court', case=False, na=False),
    df['business_name'].str.contains('bakery', case=False, na=False),
    df['business_name'].str.contains('food|kitchen|taqueria|restaurant|bistro|pizza|pizzeria', case=False, na=False),
    df['business_name'].str.contains('cafe|caffe|coffee', case=False, na=False),
    df['business_name'].str.contains('hotel|motel|lodge', case=False, na=False),
    df['business_name'].str.contains('bar|alcohol|tipsy|saloon', case=False, na=False)
]

#these are corresponding categories for the conditions
choices = ['grocery', 'school', 'bakery', 'restaurant', 'cafe', 'hotel', 'place with alcohol']

#assigns to category based off of which condition
df['category'] = np.select(conditions, choices, default='other')
print(df[['business_name', 'category']].head(70))

for choice in choices:
    print(f"\nCategory: {choice}")
    examples = df[df['category'] == choice].head(50)
    print(examples[['business_name', 'category']])

"""#data viz"""

#visualizing missing values
import missingno as msno

msno.matrix(df)
plt.show()

df = df.dropna()

import json

#I don't quite have a geojson file just a json file
with open('cal.json', 'r') as f:
    geojson_data = json.load(f);

#this is to check which one is the zip code since the key_on wasn't locating first with just properties.ZIP or properties.zipcode, etc.
#key_on links my target data (violation risk etc) to geospacial boundaries
# {
#   "type": "Feature",
#   "properties": {
#     "ZCTA5CE10": "94601",
#     "GEOID10": "0694601",
#     "ALAND10": 8410939
#   },
#  # "geometry": {
# #    "type": "Polygon",
# #    "coordinates": [
#  #     [[-122.2187, 37.7755], [-122.2171, 37.7740], ...]
# #    ]
# #  }
# #}

print(geojson_data['features'][0]['properties'])

#I've grouped everything by zipcode
violations_by_zip = df.groupby('business_postal_code')['violation_id'].count().reset_index()
violations_by_zip.columns = ['ZIP Code', 'Violation Count']

#the map center is just the mean latitude and longitude
center_lat = df['business_latitude'].mean()
center_long = df['business_longitude'].mean()

#ZIP code boundaries
zip_geo = 'cal.json'

#map creation
map = folium.Map(location=[center_lat, center_long], zoom_start=10)

#Choropleth map
folium.Choropleth(
    geo_data=zip_geo,
    name='choropleth',
    data=violations_by_zip,
    columns=['ZIP Code', 'Violation Count'],
    key_on='feature.properties.ZCTA5CE10',  #here is my key
    fill_color='YlGnBu',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Number of Violations by ZIP Code'
).add_to(map)

import pandas as pd
import folium
from folium import Choropleth
from IPython.display import IFrame
from google.colab import files

zip_geo = 'cal.json'

violations_by_zip = pd.DataFrame({
    'ZIP Code': [94102, 94103, 94104],
    'Violation Count': [20, 30, 15]
})

violations_by_zip['ZIP Code'] = violations_by_zip['ZIP Code'].astype(str).str.zfill(5)

map = folium.Map(location=[37.7749, -122.4194], zoom_start=12)

folium.Choropleth(
    geo_data=zip_geo,
    name='choropleth',
    data=violations_by_zip,
    columns=['ZIP Code', 'Violation Count'],
    key_on='feature.properties.ZCTA5CE10',
    fill_color='YlGnBu',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Number of Violations by ZIP Code'
).add_to(map)

map.save('violations_by_zip.html')
IFrame('violations_by_zip.html', width=700, height=500)

files.download('violations_by_zip.html')

#map display
#map.save('violations_by_zip.html')
#map

#from IPython.display import IFrame

#map.save('violations_by_zip.html')
#IFrame('violations_by_zip.html', width=700, height=500)

#from google.colab import files
#map.save('violations_by_zip.html')

#html file bc it's not loading properly in colab
#files.download('violations_by_zip.html')

b_94110 = df[df['business_postal_code'] == '94110'] #zipcode with highest offesnses
print(b_94110)

violations_by_zip = df.groupby('business_postal_code')['violation_id'].count().reset_index()
violations_by_zip.columns = ['ZIP Code', 'Violation Count']

plt.figure(figsize=(12, 6))
bars = plt.bar(violations_by_zip['ZIP Code'], violations_by_zip['Violation Count'], color='skyblue')

#labeling
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval, int(yval), ha='center', va='bottom')

plt.title('Number of Violations by ZIP Code', fontsize=16)
plt.xlabel('ZIP Code', fontsize=14)
plt.ylabel('Violation Count', fontsize=14)
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()
#chose not to use seaborn to have more control over presentation

"""#more viz"""

violations_summary = df.groupby(['business_postal_code', 'violation_description']).size().reset_index(name='Count')
pivot_table = violations_summary.pivot(index='violation_description', columns='business_postal_code', values='Count').fillna(0)

pivot_table = pivot_table.astype(int)
print("Number of Violations by Type and ZIP Code:")
print(tabulate(pivot_table, headers='keys', tablefmt='pretty'))

unique_violations = df['violation_description'].unique()
unique_violations_list = unique_violations.tolist()

print(unique_violations_list)

import matplotlib.pyplot as plt
import numpy as np

violation_categories = {
    'Food Safety': [
        'Inadequate food safety knowledge or lack of certified food safety manager',
        'Improper food storage',
        'Foods not protected from contamination',
        'Improper cooking time or temperatures',
        'Improper food labeling or menu misrepresentation',
        'Food safety certificate or food handler card not available',
        'Improper cooling methods',
        'Improper thawing methods',
        'Unclean or unsanitary food contact surfaces',
        'Improper reheating of food',
        'Contaminated or adulterated food'
    ],
    'Sanitation': [
        'Unclean hands or improper use of gloves',
        'Unclean nonfood contact surfaces',
        'Wiping cloths not clean or properly stored or inadequate sanitizer',
        'Inadequately cleaned or sanitized food contact surfaces',
        'Unclean or degraded floors walls or ceilings',
        'Unclean or unsanitary refuse containers or area or no garbage service',
        'Inadequate warewashing facilities or equipment'
    ],
    'Vermin Infestation': [
        'High risk vermin infestation',
        'Moderate risk vermin infestation',
        'Low risk vermin infestation'
    ],
    'Equipment Issues': [
        'Improper or defective plumbing',
        'Unapproved or unmaintained equipment or utensils',
        'No thermometers or uncalibrated thermometers',
        'Mobile food facility with unapproved operating conditions',
        'Improper storage of equipment utensils or linens'
    ],
    'Health and Safety': [
        'Inadequate and inaccessible handwashing facilities',
        'Employee eating or smoking',
        'Discharge from employee nose mouth or eye',
        'Sewage or wastewater contamination',
        'Non service animal',
        'Worker safety hazards'
    ],
    'Licensing and Permits': [
        'Permit license or inspection report not posted',
        'Unpermitted food facility',
        'No plan review or Building Permit',
        'Noncompliance with shell fish tags or display',
        'Noncompliance with HAACP plan or variance'
    ]
}

violation_mapping = {desc: idx for idx, category in enumerate(violation_categories) for desc in violation_categories[category]}
df['violation_code'] = df['violation_description'].map(violation_mapping)

violations_type_by_zip = df.groupby(['business_postal_code', 'violation_code']).size().reset_index(name='Count')

zip_codes = violations_type_by_zip['business_postal_code'].unique()
num_zip_codes = len(zip_codes)

cols = 3
rows = (num_zip_codes + cols - 1) // cols
fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))
fig.suptitle('Violation Types Distribution per ZIP Code', fontsize=20, fontweight='bold')

for i, zip_code in enumerate(zip_codes):
    subset = violations_type_by_zip[violations_type_by_zip['business_postal_code'] == zip_code]
    ax = axes[i // cols, i % cols]

    wedges, texts, autotexts = ax.pie(subset['Count'], labels=subset['violation_code'], autopct='%1.1f%%', startangle=90, radius=0.7, labeldistance=1.2)

    ax.set_title(f'{zip_code}', fontsize=16)
    ax.axis('equal')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
legend_labels = [f"{code}: {desc}" for desc, code in violation_mapping.items()]
fig.legend(legend_labels, loc='upper center', bbox_to_anchor=(0.5, 1), bbox_transform=plt.gcf().transFigure, ncol=3)

plt.show()

"""#violations/highways attempts

"""

print(df.columns)

df['violation_code'] = df['violation_description'].map(violation_mapping)

#group by ZIP code and violation code, and calculate counts
violations_type_by_zip = df.groupby(['business_postal_code', 'violation_code']).size().reset_index(name='Count')

violations_type_by_zip.to_csv('violations_type_by_zip.csv', index=False)

print(violations_type_by_zip)

#making sure they're valid codes
valid_df = df[(df['business_postal_code'].notna()) &
              (df['business_postal_code'] != '00000') &
              (df['business_postal_code'].str.isdigit())]

#putting descriptions to codes
valid_df['violation_code'] = valid_df['violation_description'].map(violation_mapping)

valid_df = valid_df[valid_df['violation_code'].notna()]

violations_type_by_zip = valid_df.groupby(['business_postal_code', 'violation_code']).size().reset_index(name='Count')
violations_type_df = pd.DataFrame(violations_type_by_zip)

print(violations_type_df)

violations_gdf = gpd.GeoDataFrame(violations_type_df,
                                  geometry=gpd.points_from_xy([122.4194, 123.4194, 124.4194], [37.7749, 38.7749, 39.7749]))

highways_gdf = gpd.read_file('roads4.shx')

violations_gdf = violations_gdf.to_crs(highways_gdf.crs)

violations_gdf['distance_to_highway'] = violations_gdf.geometry.apply(lambda x: highways_gdf.distance(x).min())

violations_gdf.to_file('violations_with_highway_proximity.geojson', driver='GeoJSON')

#.shp (the main shapefile)
#.shx (the index file)
#.dbf (the attribute data file)
#.prj (the projection file, optional but recommended)

import os

print(os.listdir('.'))

#recursively list all directories and files starting from the root
for root, dirs, files in os.walk('.'):
    print(f"Root: {root}")
    print(f"Directories: {dirs}")
    print(f"Files: {files}")

import geopandas as gpd#UTM zone 10N (EPSG:26910) for sfo projection

violations_gdf = gpd.GeoDataFrame(violations_type_df,
                                  geometry=gpd.points_from_xy([122.4194, 123.4194, 124.4194], [37.7749, 38.7749, 39.7749]))

violations_gdf.set_crs('EPSG:4326', inplace=True)

highways_gdf = gpd.read_file('tl_2020_06075_roads.shp')

# Reproject both GeoDataFrames to a projected CRS (e.g., UTM zone 10N)
violations_gdf = violations_gdf.to_crs(epsg=26910)
highways_gdf = highways_gdf.to_crs(epsg=26910)

violations_gdf['distance_to_highway'] = violations_gdf.geometry.apply(lambda x: highways_gdf.distance(x).min())

violations_gdf.to_file('violations_with_highway_proximity.geojson', driver='GeoJSON')

violations_gdf['distance_to_highway'] = violations_gdf.geometry.apply(lambda x: highways_gdf.distance(x).min())

#correlations
violations_df = violations_gdf[['business_postal_code', 'violation_code', 'Count', 'distance_to_highway']]
correlation_matrix = violations_df.corr()

print(correlation_matrix)

"""#pca"""

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

numerical_data = df.select_dtypes(include=['float64', 'int64'])

# Step 1: Standardize the Data
scaler = StandardScaler()
standardized_data = scaler.fit_transform(numerical_data)

# Step 2: Fit PCA to find the explained variance ratio of each component
pca_full = PCA()
pca_full.fit(standardized_data)

# Plot the cumulative explained variance ratio to determine the number of components to keep
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1),
         pca_full.explained_variance_ratio_.cumsum(), marker='o')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Explained Variance vs. Number of Components')
plt.grid(True)
plt.show()

# Based on the plot, choose the number of components that capture around 90-95% of variance
# (adjust `n_components` based on your plot)
n_components = 10  # Example: change this to the number that makes sense for your data
pca = PCA(n_components=n_components)
pca_data = pca.fit_transform(standardized_data)

# Step 3: Visualize the PCA Results
# Plot the first two principal components to see potential clustering or patterns
plt.figure(figsize=(10, 6))
plt.scatter(pca_data[:, 0], pca_data[:, 1], alpha=0.5, c='blue')
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.title('PCA Scatter Plot of First Two Components')
plt.grid(True)
plt.show()

# Step 4: Display the PCA Component Loadings
# These loadings indicate how much each original feature contributes to the principal components
loadings = pd.DataFrame(pca.components_.T,
                        columns=[f'PC{i+1}' for i in range(n_components)],
                        index=numerical_data.columns)
print("PCA Component Loadings:")
print(loadings)

# Optional: Save the PCA-transformed data if needed for further modeling or analysis
pca_df = pd.DataFrame(pca_data, columns=[f'PC{i+1}' for i in range(n_components)])
pca_df.to_csv('pca_transformed_data.csv', index=False)

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score

X = df.drop(columns=['inspection_score'])
y = df['inspection_score']
X = pd.get_dummies(X, drop_first=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

y_pred = rf_model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
rmse = mse ** 0.5
mae = mean_absolute_error(y_test, y_pred)
mape = (abs((y_test - y_pred) / y_test).mean()) * 100
r2 = r2_score(y_test, y_pred)
explained_variance = explained_variance_score(y_test, y_pred)

print(f"MSE: {mse:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"MAE: {mae:.2f}")
print(f"MAPE: {mape:.2f}%")
print(f"R2 Score: {r2:.2f}")
print(f"Explained Variance Score: {explained_variance:.2f}")

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import pandas as pd

label_encoder = LabelEncoder()

X = df.drop(columns=['inspection_score'])
y = df['inspection_score']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

y_pred = rf_model.predict(X_test)

precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')

cm = confusion_matrix(y_test, y_pred)

results = pd.DataFrame({
    'Metric': ['Precision', 'Recall', 'Accuracy', 'F1 Score'],
    'Value': [precision, recall, accuracy, f1]
})

print(results)
print("\nConfusion Matrix:")
print(cm)

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import pandas as pd


X = df.drop(columns=['inspection_score'])
y = df['inspection_score']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

svm_model = SVC(kernel='linear', random_state=42)
svm_model.fit(X_train, y_train)

y_pred = svm_model.predict(X_test)

precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')

cm = confusion_matrix(y_test, y_pred)

results = pd.DataFrame({
    'Metric': ['Precision', 'Recall', 'Accuracy', 'F1 Score'],
    'Value': [precision, recall, accuracy, f1]
})

print(results)
print("\nConfusion Matrix:")
print(cm)